# RAG System Architecture - Detailed Documentation

## ğŸ—ï¸ System Overview

Your RAG (Retrieval-Augmented Generation) system follows a **3-pipeline architecture**:
1. **Data Ingestion Pipeline** - PDF â†’ Chunks â†’ Embeddings â†’ Vector DB
2. **Retrieval Pipeline** - Query â†’ Embedding â†’ Similarity Search â†’ Context
3. **Generation Pipeline** - Context + Query â†’ LLM â†’ Answer

---

## ğŸ“Š Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        DATA INGESTION PIPELINE                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    ğŸ“„ PDF Files (data/pdf/)
         â”‚
         â”œâ”€â–º PyPDFLoader
         â”‚   â””â”€â–º Loads 204 pages from SAP HANA PDF
         â”‚
         â–¼
    ğŸ“‘ Documents (204 pages)
         â”‚
         â”œâ”€â–º RecursiveCharacterTextSplitter
         â”‚   â”œâ”€ chunk_size: 800 characters
         â”‚   â”œâ”€ chunk_overlap: 100 characters
         â”‚   â””â”€ separators: ["\n\n", "\n", " ", ""]
         â”‚
         â–¼
    ğŸ§© Chunks (509 chunks)
         â”‚
         â”œâ”€â–º EmbeddingManager
         â”‚   â”œâ”€ Model: all-MiniLM-L6-v2 (HuggingFace)
         â”‚   â”œâ”€ Embedding dimension: 384
         â”‚   â””â”€ Framework: SentenceTransformers
         â”‚
         â–¼
    ğŸ”¢ Embeddings (509 Ã— 384 vectors)
         â”‚
         â”œâ”€â–º VectorStore (ChromaDB)
         â”‚   â”œâ”€ Storage: Persistent (./chroma_db)
         â”‚   â”œâ”€ Collection: pdf_documents
         â”‚   â””â”€ Metadata: source, page, file_type
         â”‚
         â–¼
    ğŸ’¾ Vector Database (ChromaDB)


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         RETRIEVAL PIPELINE                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    â“ User Query
         â”‚
         â”œâ”€â–º EmbeddingManager
         â”‚   â””â”€â–º Convert query to 384-dim vector
         â”‚
         â–¼
    ğŸ”¢ Query Embedding (1 Ã— 384 vector)
         â”‚
         â”œâ”€â–º RAGRetriever
         â”‚   â”œâ”€ Similarity search in ChromaDB
         â”‚   â”œâ”€ Top-k retrieval (k=5 default)
         â”‚   â””â”€ Cosine similarity scoring
         â”‚
         â–¼
    ğŸ“š Retrieved Context (Top 5 relevant chunks)


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        GENERATION PIPELINE                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    ğŸ“š Retrieved Context + â“ User Query
         â”‚
         â”œâ”€â–º PromptTemplate
         â”‚   â””â”€â–º Formats context + question
         â”‚
         â–¼
    ğŸ“ Formatted Prompt
         â”‚
         â”œâ”€â–º OpenAILLM
         â”‚   â”œâ”€ Model: gpt-4o-mini
         â”‚   â”œâ”€ API: OpenAI Chat Completions
         â”‚   â”œâ”€ Temperature: 0.1 (factual)
         â”‚   â””â”€ Max tokens: 1024
         â”‚
         â–¼
    ğŸ’¬ Generated Answer
```

---

## ğŸ”§ Component Details

### 1. **PDF Loader**
```python
Component: PyPDFLoader (LangChain)
Input: PDF files from ../data/pdf/
Output: 204 Document objects
Metadata: source, page, total_pages, creator, etc.
```

### 2. **Text Splitter**
```python
Component: RecursiveCharacterTextSplitter
Strategy: Hierarchical splitting
Parameters:
  - chunk_size: 800 chars
  - chunk_overlap: 100 chars
  - separators: ["\n\n", "\n", " ", ""]
Input: 204 documents
Output: 509 chunks
```

### 3. **Embedding Manager**
```python
Component: SentenceTransformer
Model: all-MiniLM-L6-v2
Source: HuggingFace
Embedding Dimension: 384
Performance: Fast, lightweight
Use Case: Semantic similarity search
```

### 4. **Vector Store**
```python
Component: ChromaDB
Type: Persistent vector database
Storage Path: ./chroma_db
Collection: pdf_documents
Index Type: HNSW (approximate nearest neighbor)
Distance Metric: Cosine similarity
Total Vectors: 509 embeddings
```

### 5. **RAG Retriever**
```python
Component: Custom RAGRetriever class
Method: Similarity search
Top-k: 5 (configurable)
Scoring: Cosine distance â†’ similarity score
Output: Ranked relevant chunks
```

### 6. **LLM Generator**
```python
Component: OpenAI GPT-4o-mini
API: OpenAI Chat Completions
Temperature: 0.1 (low = factual)
Max Tokens: 1024
Prompt Strategy: Context-grounded generation
```

---

## ğŸ“ˆ Data Flow Statistics

| Stage | Input | Output | Transformation |
|-------|-------|--------|----------------|
| **PDF Loading** | 1 PDF file | 204 documents | Page extraction |
| **Chunking** | 204 documents | 509 chunks | Text splitting |
| **Embedding** | 509 chunks | 509 Ã— 384 vectors | Semantic encoding |
| **Storage** | 509 vectors | ChromaDB index | Vector indexing |
| **Retrieval** | 1 query | 5 chunks | Similarity search |
| **Generation** | Context + query | 1 answer | LLM synthesis |

---

## ğŸ” Configuration Files

### Environment Variables (.env)
```bash
OPENAI_API_KEY=sk-proj-...
```

### Dependencies (requirement.txt)
```
langchain
langchain-core
langchain-community
langchain_openai
pypdf
pymupdf
jupyter
sentence-transformers
faiss-cpu
chromadb
python-dotenv
```

---

## ğŸš€ Execution Flow

### Phase 1: Indexing (One-time setup)
```
1. Load PDFs â†’ 204 pages
2. Split into chunks â†’ 509 chunks
3. Generate embeddings â†’ 509 Ã— 384 vectors
4. Store in ChromaDB â†’ Persistent storage
```

### Phase 2: Query (Runtime)
```
1. User asks question
2. Convert query to embedding â†’ 1 Ã— 384 vector
3. Search ChromaDB â†’ Top 5 similar chunks
4. Format prompt with context
5. Send to GPT-4o-mini
6. Return generated answer
```

---

## ğŸ’¾ Storage Structure

```
/Users/prajwal/Desktop/RAG/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ pdf/
â”‚       â””â”€â”€ HA201 - SAP HANA 2.0 SPS05.pdf
â”œâ”€â”€ notebook/
â”‚   â””â”€â”€ pdf_loader.ipynb (Main RAG pipeline)
â”œâ”€â”€ chroma_db/ (Vector database - persistent)
â”‚   â””â”€â”€ [ChromaDB files]
â”œâ”€â”€ .env (API keys)
â”œâ”€â”€ requirement.txt
â””â”€â”€ .venv/ (Virtual environment)
```

---

## âš¡ Performance Characteristics

### Embedding Model (all-MiniLM-L6-v2)
- **Speed**: ~1000 sentences/sec on CPU
- **Size**: 80 MB
- **Quality**: Good for semantic search
- **Dimension**: 384 (compact)

### Vector Database (ChromaDB)
- **Query Speed**: <100ms for 509 vectors
- **Scalability**: Millions of vectors
- **Storage**: Persistent on disk
- **Index**: HNSW (fast approximate search)

### LLM (GPT-4o-mini)
- **Speed**: ~2-5 seconds per response
- **Cost**: $0.15/1M input tokens
- **Context Window**: 128K tokens
- **Quality**: High accuracy, factual

---

## ğŸ¯ RAG Strategy

### Retrieval Strategy
- **Method**: Dense vector similarity (cosine)
- **Top-k**: 5 chunks
- **Reranking**: None (can be added)
- **Filtering**: Metadata-based (optional)

### Generation Strategy
- **Temperature**: 0.1 (factual, deterministic)
- **System Prompt**: "Use ONLY the context below"
- **Max Tokens**: 1024
- **Grounding**: Strict context adherence

---

## ğŸ”„ Workflow Summary

```mermaid
graph TD
    A[PDF Files] -->|PyPDFLoader| B[204 Documents]
    B -->|RecursiveTextSplitter| C[509 Chunks]
    C -->|SentenceTransformer| D[509 Embeddings]
    D -->|ChromaDB| E[Vector Store]
    
    F[User Query] -->|Embed| G[Query Vector]
    G -->|Similarity Search| E
    E -->|Top 5 Chunks| H[Retrieved Context]
    
    H -->|Format Prompt| I[Prompt Template]
    F -->|Add Question| I
    I -->|OpenAI API| J[GPT-4o-mini]
    J -->|Generate| K[Final Answer]
```

---

## ğŸ“ Key Features

âœ… **Persistent Storage**: ChromaDB saves embeddings to disk  
âœ… **Semantic Search**: Dense embeddings capture meaning  
âœ… **Overlap Strategy**: 100-char overlap prevents context loss  
âœ… **Metadata Tracking**: Source, page numbers preserved  
âœ… **Factual Generation**: Low temperature + context grounding  
âœ… **Scalable**: Can handle thousands of documents  
âœ… **Cost-Effective**: Local embeddings + cheap LLM  

---

## ğŸ› ï¸ Future Enhancements

1. **Hybrid Search**: Combine dense + sparse (BM25) retrieval
2. **Reranking**: Add cross-encoder for better relevance
3. **Streaming**: Stream LLM responses in real-time
4. **Caching**: Cache frequent queries
5. **Multi-query**: Generate multiple query variations
6. **Evaluation**: Add RAGAS metrics for quality assessment
